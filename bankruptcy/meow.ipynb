{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data.csv\")  # replace with actual path\n",
    "print(df.head())\n",
    "\n",
    "# Check class balance\n",
    "print(df['Bankrupt?'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ‚úÖ 1. Load your dataset (replace with your actual file name)\n",
    "df = pd.read_csv(\"data.csv\")  # example: \"bankruptcy_data.csv\"\n",
    "print(df.head())  # optional: see first few rows\n",
    "\n",
    "# ‚úÖ 2. Prepare features and labels\n",
    "X = df.drop(\"Bankrupt?\", axis=1)\n",
    "y = df[\"Bankrupt?\"]\n",
    "\n",
    "# ‚úÖ 3. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# ‚úÖ 4. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Data loaded and preprocessed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Train the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüß† Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add class_weight='balanced' to give minority class more importance\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feature_importances.nlargest(10).plot(kind='barh')\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train with balanced data\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]  # top 10\n",
    "features = X.columns[indices]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(features, importances[indices])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Just replace GridSearchCV with RandomizedSearchCV in the above setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,  # ‚úÖ FIXED\n",
    "    n_iter=20,             # Number of random combinations to try\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"‚úÖ Best Parameters Found:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"üìä Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"üß† Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, \"best_bankruptcy_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist,  # ‚úÖ correct key here\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = grid_search.best_estimator_.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"üîç Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "print(\"\\nüß† Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Feature importances\n",
    "importances = grid_search.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=30,  # balance the class\n",
    "    max_depth=10,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=30,  # balance the class\n",
    "    max_depth=10,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = grid_search.best_estimator_.predict_proba(X_test_scaled)[:, 1]\n",
    "y_custom = (y_probs > 0.3).astype(int)  # try 0.3 or 0.4 instead of default 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(grid_search.best_estimator_, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class 1 explanation for sample 0 (bankruptcy)\n",
    "single_explanation = shap.Explanation(\n",
    "    values=shap_values.values[0, 1],\n",
    "    base_values=shap_values.base_values[0, 1],\n",
    "    data=shap_values.data[0],\n",
    "    feature_names=shap_values.feature_names\n",
    ")\n",
    "\n",
    "# Now plot it\n",
    "shap.plots.waterfall(single_explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_explanation = shap.Explanation(\n",
    "    values=shap_values.values[0, 1],\n",
    "    base_values=shap_values.base_values[0, 1],\n",
    "    data=X_test_scaled[0],  # OR: X_test.iloc[0] if using DataFrame\n",
    "    feature_names=X.columns.tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for class 1 (bankruptcy)\n",
    "probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Sort by highest risk\n",
    "top_indices = probs.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot SHAP explanations for top risky companies\n",
    "for idx in top_indices:\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_values.values[idx, 1],\n",
    "        base_values=shap_values.base_values[idx, 1],\n",
    "        data=X_test_scaled[idx],\n",
    "        feature_names=X.columns.tolist()\n",
    "    )\n",
    "    shap.plots.waterfall(explanation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "ticker = yf.Ticker(\"AAPL\")\n",
    "info = ticker.info\n",
    "\n",
    "# You can extract financial features like this:\n",
    "features = [\n",
    "    info.get(\"returnOnAssets\", 0),\n",
    "    info.get(\"debtToEquity\", 0),\n",
    "    info.get(\"grossMargins\", 0),\n",
    "    info.get(\"operatingMargins\", 0),\n",
    "    info.get(\"revenueGrowth\", 0),\n",
    "    info.get(\"netMargins\", 0)\n",
    "]\n",
    "\n",
    "print(\"AAPL features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Pick your stock symbols\n",
    "symbols = [\"AAPL\", \"MSFT\", \"TSLA\", \"GOOGL\"]\n",
    "\n",
    "# Fetch data\n",
    "for symbol in symbols:\n",
    "    stock = yf.Ticker(symbol)\n",
    "    \n",
    "    # Get latest info\n",
    "    info = stock.info\n",
    "    print(f\"\\nüìä {symbol} - {info['longName']}\")\n",
    "    print(f\"Current Price: ${info['regularMarketPrice']}\")\n",
    "    print(f\"52-Week Range: {info['fiftyTwoWeekLow']} - {info['fiftyTwoWeekHigh']}\")\n",
    "    print(f\"Market Cap: {info['marketCap']}\")\n",
    "    print(f\"üìà Sector: {info.get('sector', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\"TSLA\", period=\"1mo\", interval=\"1d\")\n",
    "print(data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = []\n",
    "\n",
    "for symbol in [\"TSLA\", \"AAPL\"]:\n",
    "    stock = yf.Ticker(symbol)\n",
    "    hist = stock.history(period=\"1d\")\n",
    "    latest_features = [\n",
    "        stock.info[\"returnOnAssets\"],\n",
    "        stock.info[\"debtToEquity\"],\n",
    "        stock.info[\"currentRatio\"]\n",
    "        # ‚ûï Add more if needed\n",
    "    ]\n",
    "    portfolio.append({\"symbol\": symbol, \"features\": latest_features})\n",
    "\n",
    "print(portfolio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pull S&P 500 symbols from Wikipedia\n",
    "sp500 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "symbols = sp500[\"Symbol\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yfinance as yf\n",
    "\n",
    "company_data = []\n",
    "\n",
    "for symbol in symbols[:50]:  # Limit initially to avoid rate limits!\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        info = ticker.info\n",
    "        \n",
    "        features = [\n",
    "            info.get(\"returnOnAssets\", 0),\n",
    "            info.get(\"debtToEquity\", 0),\n",
    "            info.get(\"currentRatio\", 0),\n",
    "            info.get(\"grossMargins\", 0),\n",
    "            info.get(\"quickRatio\", 0)\n",
    "            # ‚ûï Add more features that match your model\n",
    "        ]\n",
    "        \n",
    "        company_data.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"features\": features\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching {symbol}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Total Companies Processed:\", len(company_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Simulated placeholder: Load the dataset you trained on\n",
    "df = pd.read_csv(\"data.csv\")  # Replace with actual file\n",
    "X = df.drop(\"Bankrupt?\", axis=1)\n",
    "expected_features = X.columns.tolist()\n",
    "\n",
    "# ‚úÖ Assume scaler and model are already trained\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# scaler = StandardScaler().fit(X)\n",
    "# model = RandomForestClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "# ‚úÖ Example company data you want to predict on\n",
    "company_stats_dict = {\n",
    "    'ROA(C) before interest and depreciation before interest': 0.51,\n",
    "    'ROA(A) before interest and % after tax': 0.56,\n",
    "    'ROA(B) before interest and depreciation after tax': 0.55,\n",
    "    'Operating Gross Margin': 0.61,\n",
    "    'Realized Sales Gross Margin': 0.61,\n",
    "    'Operating Profit Rate': 1.0,\n",
    "    'Pre-tax net Interest Rate': 0.80,\n",
    "    'After-tax net Interest Rate': 0.81,\n",
    "    'Non-industry income and expenditure/revenue': 0.4,\n",
    "    # ... continue to fill or default the rest\n",
    "}\n",
    "\n",
    "# ‚úÖ Build feature vector of shape (1, 95) with default 0.0 if missing\n",
    "company_features = [company_stats_dict.get(col, 0.0) for col in expected_features]\n",
    "X_input = np.array(company_features).reshape(1, -1)\n",
    "\n",
    "# ‚úÖ Scale and predict\n",
    "X_input_scaled = scaler.transform(X_input)\n",
    "prediction = model.predict(X_input_scaled)[0]\n",
    "prediction_prob = model.predict_proba(X_input_scaled)[0][1]\n",
    "\n",
    "# ‚úÖ Output result\n",
    "print(\"üîé Bankruptcy Risk Prediction:\", \"‚ö†Ô∏è High Risk\" if prediction == 1 else \"‚úÖ Low Risk\")\n",
    "print(f\"üìä Risk Score: {prediction_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Save model (e.g., RandomForest, XGBoost, etc.)\n",
    "joblib.dump(model, \"bankruptcy_model.pkl\")\n",
    "\n",
    "print(\"‚úÖ Scaler and model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ‚úÖ Load training feature structure\n",
    "df = pd.read_csv(\"data.csv\")  # Use the same dataset you trained on\n",
    "X = df.drop(\"Bankrupt?\", axis=1)\n",
    "expected_features = X.columns.tolist()\n",
    "\n",
    "# ‚úÖ Load trained scaler and model\n",
    "scaler = joblib.load(\"scaler.pkl\")              # Must have been saved using joblib.dump()\n",
    "model = joblib.load(\"bankruptcy_model.pkl\")     # Same as above\n",
    "\n",
    "# ‚úÖ Example company financial stats (partial data shown here)\n",
    "company_stats_dict = {\n",
    "    'ROA(C) before interest and depreciation before interest': 0.51,\n",
    "    'ROA(A) before interest and % after tax': 0.56,\n",
    "    'ROA(B) before interest and depreciation after tax': 0.55,\n",
    "    'Operating Gross Margin': 0.61,\n",
    "    'Realized Sales Gross Margin': 0.61,\n",
    "    'Operating Profit Rate': 1.0,\n",
    "    'Pre-tax net Interest Rate': 0.80,\n",
    "    'After-tax net Interest Rate': 0.81,\n",
    "    'Non-industry income and expenditure/revenue': 0.4,\n",
    "    # ‚ûï Add remaining features or let them default to 0.0\n",
    "}\n",
    "\n",
    "# ‚úÖ Build feature vector of shape (1, 95)\n",
    "company_features = [company_stats_dict.get(col, 0.0) for col in expected_features]\n",
    "X_input_df = pd.DataFrame([company_features], columns=expected_features)\n",
    "\n",
    "# ‚úÖ Scale and predict\n",
    "X_input_scaled = scaler.transform(X_input_df)\n",
    "prediction = model.predict(X_input_scaled)[0]\n",
    "prediction_prob = model.predict_proba(X_input_scaled)[0][1]\n",
    "\n",
    "# ‚úÖ Output result\n",
    "print(\"\\nüîé Bankruptcy Risk Prediction:\", \"‚ö†Ô∏è High Risk\" if prediction == 1 else \"‚úÖ Low Risk\")\n",
    "print(f\"üìä Risk Score: {prediction_prob:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load the dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"data.csv\")  # Update path if needed\n",
    "X = df.drop(\"Bankrupt?\", axis=1)\n",
    "y = df[\"Bankrupt?\"]\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Train/test split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Preprocessing\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save for future use\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Train Model\n",
    "# -------------------------------\n",
    "model = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"bankruptcy_model.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: Use a real row from data\n",
    "# -------------------------------\n",
    "# Let's say row 100\n",
    "sample_index = 150\n",
    "sample_features = X.iloc[sample_index].values.reshape(1, -1)\n",
    "sample_scaled = scaler.transform(sample_features)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 6: Predict\n",
    "# -------------------------------\n",
    "prediction = model.predict(sample_scaled)[0]\n",
    "prediction_prob = model.predict_proba(sample_scaled)[0][1]\n",
    "\n",
    "print(f\"\\nüì¶ Company #{sample_index}\")\n",
    "print(\"üîé Bankruptcy Risk Prediction:\", \"‚ö†Ô∏è High Risk\" if prediction == 1 else \"‚úÖ Low Risk\")\n",
    "print(f\"üìä Risk Score: {prediction_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 1: Load model, scaler, and features\n",
    "# ------------------------------\n",
    "model = joblib.load(\"bankruptcy_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "expected_features = joblib.load(\"features_list.pkl\")  # Make sure you saved this during training\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 2: Partial company input (only a few features)\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load the dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"data.csv\")  # Update path if needed\n",
    "X = df.drop(\"Bankrupt?\", axis=1)\n",
    "y = df[\"Bankrupt?\"]\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Train/test split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Preprocessing\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save for future use\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Train Model\n",
    "# -------------------------------\n",
    "model = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 3: Build full-length feature vector\n",
    "# ------------------------------\n",
    "company_features = [company_stats_dict.get(feature, 0.0) for feature in expected_features]\n",
    "X_input = np.array(company_features).reshape(1, -1)\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 4: Scale and Predict\n",
    "# ------------------------------\n",
    "X_input_scaled = scaler.transform(X_input)\n",
    "prediction = model.predict(X_input_scaled)[0]\n",
    "prediction_prob = model.predict_proba(X_input_scaled)[0][1]\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 5: Output\n",
    "# ------------------------------\n",
    "print(\"üì¶ Company: [Partial data input]\")\n",
    "print(\"üîé Bankruptcy Risk Prediction:\", \"‚ö†Ô∏è High Risk\" if prediction == 1 else \"‚úÖ Low Risk\")\n",
    "print(f\"üìä Risk Score: {prediction_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# for company in company_data:\n",
    "#     try:\n",
    "#         X_input = np.array(company[\"features\"]).reshape(1, -1)\n",
    "#         X_input_scaled = scaler.transform(X_input)\n",
    "#         prediction = model.predict(X_input_scaled)[0]\n",
    "\n",
    "#         if prediction == 1:\n",
    "#             print(f\"üö® {company['symbol']} ‚Üí ‚ö†Ô∏è High bankruptcy risk!\")\n",
    "#         else:\n",
    "#             print(f\"‚úÖ {company['symbol']} ‚Üí üü¢ Stable\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {company['symbol']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler.fit(X_train_scaled)  # X_train is your training features before scaling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bankruptcy-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
